#Processnig	 #Q1(a)  How is the data structured? Draw a directory tree to represent this in a sensible way. #get acress to the datahdfs dfs -ls /data/ghcnd/hdfs dfs -ls /data/ghcnd/daily#(b)  How many years are contained in daily, and how does the size of the data change? #count of the number of files in daily hdfs dfs -ls /data/ghcnd/daily | wc -l#get the tree directoryhadoop fs -lsr /data/ghcnd | awk '{print $8}' | \sed -e 's/[^-][^\/]*\//--/g' -e 's/^/ /' -e 's/-/|/'#(c)  What is the total size of all of the data? How much of that is daily? #get the size of the data hadoop fs -du -s /data/ghcnd/ | awk '{s+=$1} END {printf  "%.3fG\n", s/1073741824}'hadoop fs -du -h /data/ghcnd/hadoop fs -du -s -h /data/ghcnd/ #human readable version #get the size of the daily data hadoop fs -du -s -h /data/ghcnd/daily | awk '{s+=$1} END {printf  "%.3fG\n", s/1073741824}' #Q2 (a)  Define schemas for each of daily, stations, states, countries, and inventory based on the descriptions in this assignment and the GHCN Daily README. These should use the data types defined in pyspark.sql. #start pyspark in terminalstart_pyspark_shell# Importsfrom pyspark import SparkContextfrom pyspark.sql import SparkSession, functions as Ffrom pyspark.sql.types import *# Define schemas for dailyschema_daily = StructType([    StructField('ID', StringType()),    StructField('DATE', StringType()),    StructField('ELEMENT', StringType()),    StructField('VALUE', IntegerType()),    StructField('MEASUREMENT_FLAG', StringType()),    StructField('QUALITY_FLAG', StringType()),    StructField('SOURCE_FLAG', StringType()),    StructField('OBSERVATION_TIME', StringType()),]) # define schemas for stationsschema_stations = StructType([    StructField('ID', StringType()),    StructField('LATITUDE', DoubleType()),    StructField('LONGITUDE', DoubleType()),    StructField('ELEVATION', DoubleType()),    StructField('STATE_CODE', StringType()),    StructField('NAME', StringType()),    StructField('GSN_FLAG', StringType()),    StructField('HCN/CRN_FLAG', StringType()),    StructField('WMO_ID', StringType()),    ]) # define schemas for countries schema_countries = StructType([    StructField('CODE', StringType()),    StructField('NAME', StringType()),])# define schemas for statesschema_states = StructType([  StructField('CODE', StringType()),  StructField('NAME', StringType()),])#define schemas for inventory schema_inventory = StructType([    StructField('ID', StringType()),    StructField('LATITUDE', DoubleType()),    StructField('LONGITUDE', DoubleType()),    StructField('ELEMENT', StringType()),    StructField('FIRSTYEAR', IntegerType()),    StructField('LASTYEAR', IntegerType()), ])#check the schemas in different files/directory!hdfs dfs -cat /data/ghcnd/countries | head!hdfs dfs -cat /data/ghcnd/inventory | head!hdfs dfs -cat /data/ghcnd/states | head!hdfs dfs -cat /data/ghcnd/stations | head!hdfs dfs -cat /data/ghcnd/daily/2020.csv.gz | gunzip | head #(b)  Load 1000 rows of hdfs:///data/ghcnd/daily/2020.csv.gz into Spark using the limit command immediately after the read command. #Was the description of the data accurate? Was there anything unexpected?  df_daily = spark.read.format("csv").option("header", "true").schema(schema_daily).load("hdfs:///data/ghcnd/daily/2020.csv.gz").limit(1000)df_daily.show() #show the first 20 rows in df #(c)  Load each of stations, states, countries, and inventory into Spark as well. You will need to find a way to parse the fixed width text formatting, as this format is not included in the standard spark.read library. You could try using spark.read.format('text') and pyspark.sql.functions.substring or finding an existing open source library. #How many rows are in each metadata table? How many stations do not have a WMO ID? #load data with fixed width text formatting df_stations=spark.read.format("text").load("hdfs:///data/ghcnd/stations")df_stations=spark.read.format("text").load("hdfs:///data/ghcnd/stations")stations=df_stations.select( F.trim(F.substring(F.col('value'), 1, 11)).alias('ID').cast(schema_stations['ID'].dataType),    F.trim(F.substring(F.col('value'), 13, 8)).alias('LATITUDE').cast(schema_stations['LATITUDE'].dataType),    F.trim(F.substring(F.col('value'), 22, 9)).alias('LONGITUDE').cast(schema_stations['LONGITUDE'].dataType),    F.trim(F.substring(F.col('value'), 32, 6)).alias('ELEVATION').cast(schema_stations['ELEVATION'].dataType),    F.trim(F.substring(F.col('value'), 39, 2)).alias('STATE_CODE').cast(schema_stations['STATE_CODE'].dataType),    F.trim(F.substring(F.col('value'), 42, 30)).alias('STATION_NAME').cast(schema_stations['NAME'].dataType),    F.trim(F.substring(F.col('value'), 73, 3)).alias('GSN_FLAG').cast(schema_stations['GSN_FLAG'].dataType),    F.trim(F.substring(F.col('value'), 77, 3)).alias('HCN/CRN_FLAG').cast(schema_stations['HCN/CRN_FLAG'].dataType),    F.trim(F.substring(F.col('value'), 81, 5)).alias('WMO_ID').cast(schema_stations['WMO_ID'].dataType))stations.show()  df_countries=spark.read.format("text").load("hdfs:///data/ghcnd/countries")countries=df_countries.select(    F.trim(F.substring(F.col('value'), 1, 2)).alias('CODE').cast(schema_countries['CODE'].dataType),    F.trim(F.substring(F.col('value'), 4, 47)).alias('NAME').cast(schema_countries['NAME'].dataType))countries.show()   df_states=spark.read.format("text").load("hdfs:///data/ghcnd/states")states=df_states.select(    F.trim(F.substring(F.col('value'), 1, 2)).alias('CODE').cast(schema_states['CODE'].dataType),    F.trim(F.substring(F.col('value'), 4, 47)).alias('NAME').cast(schema_states['NAME'].dataType))states.show()   df_inventory=spark.read.format("text").load("hdfs:///data/ghcnd/inventory")inventory=df_inventory.select(    F.trim(F.substring(F.col('value'), 1, 11)).alias('ID').cast(schema_inventory['ID'].dataType),    F.trim(F.substring(F.col('value'), 13, 8)).alias('LATITUDE').cast(schema_inventory['LATITUDE'].dataType),    F.trim(F.substring(F.col('value'), 22, 9)).alias('LONGITUDE').cast(schema_inventory['LONGITUDE'].dataType),    F.trim(F.substring(F.col('value'), 32, 4)).alias('ELEMENT').cast(schema_inventory['ELEMENT'].dataType),    F.trim(F.substring(F.col('value'), 37, 4)).alias('FIRSTYEAR').cast(schema_inventory['FIRSTYEAR'].dataType),    F.trim(F.substring(F.col('value'), 42, 4)).alias('LASTYEAR').cast(schema_inventory['LASTYEAR'].dataType))inventory.show() #we've created a new dataframe called stations with the splited data from the fixed width text format#get the count of stations without a WMO ID stations.filter(stations.WMO_ID=="").count() #Q3.(a)  Extract the two character country code from each station code in stations and store the output as a new column using the withColumn command. #extract the two character country code from each station code and store the output as a new column named 'COUNTRY_CODE'stations = stations.withColumn('COUNTRY_CODE', F.substring((stations.ID),1,2))#(b)  LEFT JOIN stations with countries using your output from part (a).  stations_countries=stations.join(countries, stations.COUNTRY_CODE==countries.CODE, 'left').withColumnRenamed('NAME', 'COUNTRY').drop('CODE') #(c)  LEFT JOIN stations and states, allowing for the fact that state codes are only provided for stations in the US. stations_countries_states=stations_countries.join(states, stations_countries.STATE_CODE==states.CODE, 'left').withColumnRenamed('NAME', 'STATE').drop('CODE')#(d)  Based on inventory, what was the first and last year that each station was active and collected any element at all? stations_active=inventory.groupby('ID').agg(F.min('FIRSTYEAR'),F.max('LASTYEAR')) #How many different elements has each station collected overall? stations_elements=inventory.select('ID','ELEMENT').dropDuplicates().groupby('ID').agg({'ELEMENT':'count'}).select('ID', F.col('count(ELEMENT)').alias('NUM_ELEMENT'))stations_elements.count() #115072 #Further, count separately the number of core elements and the number of ”other” elements that each station has collected overall. inventory_core = inventory.filter(inventory.ELEMENT.isin(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN']))inventory_core.count() #318583inventory.count() #687141inventory_core.groupby('ELEMENT').count().show()#How many stations collect all five core elements? How many only collected temperature? inventory_element=stations_elements.join(inventory_core,on='ID',how='left') #join inventory with core elements and the number of elements to get a table that contains the total of all elementsinventory_element_core=inventory_core.select('ID','ELEMENT').dropDuplicates().groupby('ID').agg({'ELEMENT':'count'}).select('ID', F.col('count(ELEMENT)').alias('CORE_ELEMENT')) #count the number of core elementsstations_num_elements=stations_elements.join(inventory_element_core,on='ID',how='left')stations_num_elements.show() #get a table that contains the count of elements and core elements separatelystations_num_elements = (stations_num_elements.withColumn('OTHER_ELEMENT',F.col('NUM_ELEMENT')-F.col('CORE_ELEMENT')))stations.show() #this gives us an extra column to display the count of other elements in each station#to confirm  the  total  number of elementsstations_num_elements.groupBy().sum('NUM_ELEMENT').collect() #count of all elements that have been collected stations_num_elements.groupBy().sum('CORE_ELEMENT').collect() #count of all core elements that have been collected #get the count of stations with 5 core elements stations_num_elements.filter(F.col('CORE_ELEMENT')==5).count()#get the count of stations with only 1 element and its temperatureelements_stations=inventory.join(stations,on='ID',how='left')elements_stations.filter(F.col('CORE_ELEMENT')==1).groupBy('ELEMENT').agg({'ELEMENT':'count'}).filter(elements_stations.ELEMENT.isin(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN'])).show() #shows the count of each element when there's only one element that was collected#tmax=elements_stations.filter(F.col('ELEMENT')=='TMAX').filter(F.col('CORE_ELEMENT')==2) #get a table that contains two core elements and one of them is TMAX#tmin=elements_stations.filter(F.col('ELEMENT')=='TMIN').filter(F.col('CORE_ELEMENT')==2) #get a table that contains two core elements and one of them is TMIN#inner=tmin.join(tmax,on='ID',how='inner').drop(tmin.CORE_ELEMENT) #inner join tmin and tmax to get the matched values  in these two tables#inner.count()#we could also use the following codes and add the two outputsstations_num_elements.filter(F.col('NUM_ELEMENT')==1).filter(F.col('ELEMENT')=="TMAX").count() stations_num_elements.filter(F.col('NUM_ELEMENT')==1).filter(F.col('ELEMENT')=="TMIN").count()#(e)  LEFT JOIN stations and your output from part (d). enriched_stations=stations.join(stations_num_elements,on='ID',how='left').drop(stations.NUM_ELEMENT).drop(stations.CORE_ELEMENT).drop(stations.OTHER_ELEMENT)enrich_stations=enriched_stations.join(stations_active,on='ID',how='left')enrich_stations.write.format('parquet').mode("overwrite").save('hdfs:///user/mga113/outputs/ghcnd/'+'stations.parquet')!hdfs dfs -ls /user/mga113/outputs#This enriched stations table will be useful. Save it to your output directory. Think carefully about the file format that you use (e.g. csv, csv.gz, parquet) with respect to consistency and efficiency. From now on assume that stations refers to this enriched table with all the new columns included. #(f)  LEFT JOIN your 1000 rows subset of daily and your output from part (e). Are there any stations in your subset of daily that are not in stations at all? df_daily=df_daily.drop('ELEMENT')daily_stations=df_daily.join(stations,on='ID',how='left')daily_stations.filter(F.col('STATION_NAME')=='').count()#How expensive do you think it would be to LEFT JOIN all of daily and stations? Could you determine if there are any stations in daily that are not in stations without using LEFT JOIN? df_daily.select(['ID']).dropDuplicates().subtract(stations.select(['ID'])).count()#Q1 First it will be helpful to know more about the stations in our database, before we study the actual daily climate summaries in more detail.#(a) How many stations are there in total?df=spark.read.parquet('/user/mga113/outputs/ghcnd/stations.parquet')df.count() #How many stations have been active in 2020?df.filter((F.col('first')<=2020) & (F.col('last')>=2020)).count() #32850#How many stations are in each of the GCOS Surface Network (GSN)df.filter(df.GSN_FLAG=='GSN').count()#the US Historical Climatology Network (HCN), df.filter(F.col('HCN/CRN_FLAG')=='HCN').count()#and the US Climate Reference Network (CRN)?df.filter(F.col('HCN/CRN_FLAG')=='CRN').count()#Are there any stations that are in more than one of these networks?df.filter(F.col('HCN/CRN_FLAG')=='HCN').filter(df.GSN_FLAG=='GSN').count() #14#(b) Count the total number of stations in each country, and store the output in countries using the withColumnRenamed command.each_country=df.groupBy('COUNTRY_CODE').agg({'COUNTRY_CODE':'count'})joined_countries=countries.join(each_country, countries.CODE==each_country.COUNTRY_CODE, how='left').drop('COUNTRY_CODE').withColumnRenamed('count(COUNTRY_CODE)', 'EACH_COUNTRY')joined_countries.show()#Do the same for states and save a copy of each table to your output directory.each_state=df.groupBy('STATE_CODE').agg({'STATE_CODE':'count'})joined_states=states.join(each_state, states.CODE==each_state.STATE_CODE, how='left').drop('STATE_CODE').withColumnRenamed('count(STATE_CODE)', 'EACH_STATE')joined_states.show()joined_states.write.format('parquet').mode("overwrite").save('hdfs:///user/mga113/outputs/ghcnd/'+'states.parquet')joined_countries.write.format('parquet').mode("overwrite").save('hdfs:///user/mga113/outputs/ghcnd/'+'countries.parquet')#(c) How many stations are there in the Northern Hemisphere only?df.filter(df.LATITUDE>0).count() #89745us_territories=joined_countries.filter(joined_countries.NAME.like('%United States%'))us_territories.groupBy().sum('EACH_COUNTRY').collect()#62183#(a) Write a Spark function that computes the geographical distance between two stations using their latitude and longitude as arguments.from pyspark.sql.functions import udfimport mathdef distance(lat1, lon1, lat2, lon2):    radius = 6371 # km    dlat = math.radians(lat2-lat1)    dlon = math.radians(lon2-lon1)    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))    d = radius * c    return ddistance_udf = udf(distance)#(b) Apply this function to compute the pairwise distances between all stations in New Zealand, and save the result to your output directory.stations=spark.read.parquet('/user/mga113/outputs/ghcnd/stations.parquet') #load  the enriched stations datastations_NZ = stations.filter(stations.COUNTRY_CODE=='NZ').select(['ID', 'LATITUDE', 'LONGITUDE']) #filter the stations in nz stations_NZ.show()stations_NZ1 = (stations_NZ.withColumnRenamed('ID', 'ID_1').withColumnRenamed('LATITUDE', 'LATITUDE_1').withColumnRenamed('LONGITUDE', 'LONGITUDE_1'))stations_NZ2 = (stations_NZ.withColumnRenamed('ID', 'ID_2').withColumnRenamed('LATITUDE', 'LATITUDE_2').withColumnRenamed('LONGITUDE', 'LONGITUDE_2'))stationNZ_pairs = (stations_NZ1.crossJoin(stations_NZ2).filter(F.col('ID_1') != F.col('ID_2')))stationNZ_with_distance = (	stationNZ_pairs	.withColumn('distance', 		distance_udf('LATITUDE_1', 'LONGITUDE_1', 'LATITUDE_2', 'LONGITUDE_2')		)	)stationNZ_with_distance=stationNZ_with_distance.distinct() #drop the duplicates stationNZ_with_distance.show()#What two stations are the geographically furthest apart in New Zealand?stationNZ_with_distance.orderBy(stationNZ_with_distance.distance.desc()).show() #sort distance reversedstationNZ_with_distance.write.format('parquet').mode('overwrite').save('hdfs:///user/mga113/outputs/ghcnd/'+"stationNZ_with_distance.parquet")#q3hdfs getconf -confKey "dfs.blocksize"#to determine the default blocksize of HDFS#determine the size of files under a specific path in HDFS.hdfs fsck hdfs:///data/ghcnd/daily/2020.csv.gz -blockshdfs fsck hdfs:///data/ghcnd/daily/2015.csv.gz -blockshdfs fsck hdfs:///data/ghcnd/daily/2016.csv.gz -blocks#(b) Load and count the number of observations in daily for each of the years 2015 and 2020.from pyspark import SparkContextfrom pyspark.sql import SparkSession, functions as Ffrom pyspark.sql.types import *schema_daily = StructType([    StructField('ID', StringType()),    StructField('DATE', StringType()),    StructField('ELEMENT', StringType()),    StructField('VALUE', IntegerType()),    StructField('MEASUREMENT_FLAG', StringType()),    StructField('QUALITY_FLAG', StringType()),    StructField('SOURCE_FLAG', StringType()),    StructField('OBSERVATION_TIME', StringType()),])#load 2015daily_2015 = spark.read.format("csv").option("header", "true").schema(schema_daily).load("hdfs:///data/ghcnd/daily/2015.csv.gz")#count the  observations of 2015daily_2015.count()#load 2020 daily_2020 = spark.read.format("csv").option("header", "true").schema(schema_daily).load("hdfs:///data/ghcnd/daily/2020.csv.gz")#count the  observations of 2020daily_2020.count()#(b) Load and count the number of observations in daily for each of the years 2015 and 2020.# count the observations from  2015 to 2020daily_15_20 = (    spark.read.format("com.databricks.spark.csv")    .option("header", "false")    .option("inferSchema", "false")    .schema(schema_daily)    .load("hdfs:///data/ghcnd/daily/20{15,16,17,18,19,20}.csv.gz"))daily_15_20.count()#q4(a) Count the number of rows in daily.df_daily = spark.read.format("csv").option("header", "true").schema(schema_daily).load("hdfs:///data/ghcnd/daily")df_daily.count()#(b)Filter daily using the filter command to obtain the subset of observations containing the five core elements described in inventory.daily_core_elements = df_daily.filter(df_daily.ELEMENT.isin(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN']))daily_core_elements.show()#How many observations are there for each of the five core elements?daily_num_core_elements = daily_core_elements.groupby('ELEMENT').count()daily_num_core_elements.show()df.groupBy('STATE_CODE').agg({'STATE_CODE':'count'})#(c) Many stations collect TMAX and TMIN, but do not necessarily report them simultaneously due to issues with data collection or coverage. #Determine how many observations of TMIN do not have a corresponding observation of TMAX.daily_temperature=df_daily.filter(df_daily.ELEMENT.isin(['TMAX','TMIN'])).groupby(['ID', 'DATE']).agg(F.collect_set('ELEMENT').alias('TEMPERATURE'))daily_temperature.show()daily_tmin = daily_temperature.filter((F.array_contains('TEMPERATURE', 'TMIN')) & (~ F.array_contains('TEMPERATURE', 'TMAX')))daily_tmin.show()daily_tmin.count()#How many different stations contributed to these observations?c#Filter daily to obtain all observations of TMIN and TMAX for all stations in New Zealand, and save the result to your output directorystations=spark.read.parquet('/user/mga113/outputs/ghcnd/stations.parquet')stations_NZ= stations.filter(stations.COUNTRY_CODE=='NZ').select(['ID'])temperature_NZ=df_daily.filter(df_daily.ELEMENT.isin(['TMAX','TMIN'])).join(stations_NZ,on='ID',how='inner')temperature_NZ.show()#How many observations are there, temperature_NZ.count()#how many years are covered temperature_NZ = temperature_NZ.withColumn('YEAR', F.trim(F.substring(F.col('DATE'), 1, 4)).cast(StringType()))temperature_NZ.dropDuplicates(['YEAR']).show()temperature_NZ.write.format('parquet').mode("overwrite").save('hdfs:///user/mga113/outputs/ghcnd/'+'stations_NZ.parquet'!hdfs dfs -ls /user/mga113/outputs/ghcnd/!hadoop fs -mv '/user/mga113/outputs/ghcnd/stations_NZ.parquet' '/user/mga113/outputs/ghcnd/temperature_NZ.parquet' #rename the file!hdfs dfs -copyToLocal /user/mga113/outputs/ghcnd/temperature_NZ.parquettemperature_NZ = spark.read.parquet("/user/mga113/outputs/ghcnd/temperature_NZ.parquet")temperature_NZ.write.csv("/user/mga113/outputs/ghcnd/temperature_NZ.csv")!hdfs dfs -copyToLocal /user/mga113/outputs/ghcnd/temperature_NZ.csv!cat temperature_NZ.csv/*.csv > temperature.csv!ls temperature.csv | wc -l#(e) Group the precipitation observations by year and country. Compute the average rainfall in each year for each country, and save this result to your output directory.#df_daily = (spark.read.format("com.databricks.spark.csv").option("header", "false").option("inferSchema", "false").schema(schema_daily).load("hdfs:///data/ghcnd/daily/*.*"))daily = df_daily.withColumn('YEAR', F.trim(F.substring(F.col('DATE'), 1, 4)).cast(StringType())) #get the year value from dateprecipitation_country = daily.filter(daily.ELEMENT=='PRCP').join(stations, on='ID', how= 'left').groupby(F.col('COUNTRY_CODE'), F.col('YEAR')).agg(F.mean('VALUE').alias('AVGPRECIPITATION')).sort(F.col('AVGPRECIPITATION'), ascending=False)precipitation_country.show()precipitation_country=precipitation_country.join(countries,countries.CODE==precipitation_country.COUNTRY_CODE, how='left').drop('COUNTRY_CODE')precipitation_country.write.format('parquet').mode('overwrite').save('hdfs:///user/mga113/outputs/ghcnd/'+"precipitation_country.parquet")precipitation_country.write.csv("/user/mga113/outputs/ghcnd/precipitation_country.csv")#Which country has the highest average rainfall in a single year across the entire dataset?precipitation_each_country = precipitation_country.groupby(F.col('CODE')).agg(F.mean('AVGPRECIPITATION').alias('PRECIPITATION'))precipitation_each_country.sort('PRECIPITATION', ascending=False).show()!hdfs dfs -copyToLocal /user/mga113/outputs/ghcnd/precipitation_country.csv!cat precipitation_country.csv/*.csv > precipitation.csv!cat precipitation_each_country.csv/*.csv > precipitation_each_country.csvprecipitation_each_country.write.csv("/user/mga113/outputs/ghcnd/precipitation_each_country.csv")#when l found out country names werent in precipitation_countr file,l had to do extra work to include country name in precipitation_country.precipitation_NZ = spark.read.parquet("/user/mga113/outputs/ghcnd/precipitation_country.parquet")precipitation_NZ.write.csv("/user/mga113/outputs/ghcnd/precipitation_country.csv")countries=spark.read.parquet('/user/mga113/outputs/ghcnd/countries.parquet')countries.show()precipitation_NZ=spark.read.csv("/user/mga113/outputs/ghcnd/precipitation_country.csv").toDF("CODE", "YEAR", "PRECIPITATION")precipitation_NZ.show()precipitation_NZ=precipitation_NZ.join(countries,on="CODE",how='left').drop("EACH_COUNTRY")precipitation_NZ.show()precipitation_NZ.write.format('parquet').mode('overwrite').save('hdfs:///user/mga113/outputs/ghcnd/'+"precipitation_NZ.csv")hdfs dfs -rm -r /user/mga113/outputs/ghcnd/precipitation_country.csv$ cat M:\precipitation_country.csv/to/files/*.csv > precipitation.csv!hdfs dfs -copyToLocal /user/mga113/outputs/ghcnd/precipitation_country.csvprecipitation_NZ=spark.read.csv("/user/mga113/outputs/ghcnd/precipitation_NZ.csv")#challenge q1 Explore the daily data in more detail.df_daily = spark.read.format("csv").option("header", "true").schema(schema_daily).load("hdfs:///data/ghcnd/daily")daily_element=df_daily.groupby('ELEMENT').count()daily_element.sort("count",ascending=False).show()daily_element.sort("count").show()daily_element.groupBy().sum().collect()daily_quality=df_daily.groupby('QUALITY_FLAG').count()daily_quality.sort("count",ascending=False).show()#How many rows in daily failed each of the quality assurance checks?#not fail df_daily.filter(df_daily.QUALITY_FLAG=="").count()stations=spark.read.parquet('/user/mga113/outputs/ghcnd/stations.parquet')stations_eu=stations.filter(stations.COUNTRY_CODE=='EK').select(['ID'])